"""
å¯¹è¯ç®¡ç†å™¨ - åè°ƒè¯­éŸ³è¯†åˆ«ã€LLM å¯¹è¯ã€è¯­éŸ³åˆæˆå’Œ Live2D åŠ¨ç”»
"""

import asyncio
import logging
import re
import time
from typing import Optional, List, Dict, Any, Callable
from dataclasses import dataclass
from enum import Enum


class ConversationState(Enum):
    """å¯¹è¯çŠ¶æ€"""
    IDLE = "idle"
    LISTENING = "listening"
    PROCESSING = "processing"
    RESPONDING = "responding"
    SPEAKING = "speaking"
    ERROR = "error"


@dataclass
class ConversationTurn:
    """å¯¹è¯è½®æ¬¡"""
    user_input: str
    assistant_response: str
    timestamp: float
    asr_latency: float = 0.0
    llm_latency: float = 0.0
    tts_latency: float = 0.0
    total_latency: float = 0.0
    emotion_detected: Optional[str] = None


class ConversationManager:
    """
    å¯¹è¯ç®¡ç†å™¨
    
    è´Ÿè´£åè°ƒå’Œç®¡ç†æ•´ä¸ªå¯¹è¯æµç¨‹ï¼š
    1. è¯­éŸ³ç›‘å¬å’Œè¯†åˆ«
    2. æƒ…æ„Ÿåˆ†æå’Œè§£æ
    3. LLM å¯¹è¯ç”Ÿæˆ
    4. è¯­éŸ³åˆæˆ
    5. Live2D åŠ¨ç”»åŒæ­¥
    """
    
    def __init__(self, asr, tts, llm, live2d_manager):
        self.asr = asr
        self.tts = tts
        self.llm = llm
        self.live2d_manager = live2d_manager
        
        self.logger = logging.getLogger(__name__)
        self.state = ConversationState.IDLE
        
        # å¯¹è¯å†å²
        self.conversation_history: List[ConversationTurn] = []
        self.max_history_turns = 10
        
        # å®æ—¶çŠ¶æ€
        self._current_turn: Optional[ConversationTurn] = None
        self._listening_task: Optional[asyncio.Task] = None
        self._processing_task: Optional[asyncio.Task] = None
        self._speaking_task: Optional[asyncio.Task] = None
        
        # é…ç½®
        self.voice_threshold = 0.5
        self.silence_timeout = 2.0  # é™éŸ³è¶…æ—¶ç§’æ•°
        self.max_speech_duration = 30.0  # æœ€å¤§è¯­éŸ³æ—¶é•¿
        
        # æƒ…æ„Ÿæ˜ å°„
        self.emotion_mapping = {
            "ğŸ˜Š": {"expression": "happy", "motion_group": "Happy"},
            "ğŸ¤©": {"expression": "excited_star", "motion_group": "Excited"},
            "ğŸ˜": {"expression": "cool", "motion_group": "Confident"},
            "ğŸ˜": {"expression": "smug", "motion_group": "Confident"},
            "ğŸ’ª": {"expression": "determined", "motion_group": "Confident"},
            "ğŸ˜³": {"expression": "embarrassed", "motion_group": "Nervous"},
            "ğŸ˜²": {"expression": "shocked", "motion_group": "Surprised"},
            "ğŸ¤”": {"expression": "thinking", "motion_group": "Thinking"},
            "ğŸ‘€": {"expression": "suspicious", "motion_group": "Thinking"},
            "ğŸ˜¤": {"expression": "frustrated", "motion_group": "Angry"},
            "ğŸ˜¢": {"expression": "sad", "motion_group": "Sad"},
            "ğŸ˜…": {"expression": "awkward", "motion_group": "Nervous"},
            "ğŸ™„": {"expression": "dismissive", "motion_group": "Annoyed"},
            "ğŸ’•": {"expression": "adoring", "motion_group": "Happy"},
            "ğŸ˜‚": {"expression": "laughing", "motion_group": "Happy"},
            "ğŸ”¥": {"expression": "passionate", "motion_group": "Excited"},
            "âœ¨": {"expression": "sparkle", "motion_group": "Happy"},
        }
        
        # äº‹ä»¶å›è°ƒ
        self.on_state_changed: Optional[Callable[[ConversationState], None]] = None
        self.on_user_speech: Optional[Callable[[str], None]] = None
        self.on_assistant_response: Optional[Callable[[str], None]] = None
        self.on_emotion_detected: Optional[Callable[[str], None]] = None
        self.on_turn_completed: Optional[Callable[[ConversationTurn], None]] = None
    
    async def start_conversation_loop(self):
        """å¯åŠ¨å¯¹è¯å¾ªç¯"""
        self.logger.info("å¯åŠ¨å¯¹è¯ç®¡ç†å™¨")
        self._set_state(ConversationState.IDLE)
        
        while True:
            try:
                if self.state == ConversationState.IDLE:
                    # å¼€å§‹ç›‘å¬
                    self._set_state(ConversationState.LISTENING)
                    self._listening_task = asyncio.create_task(self._listen_for_speech())
                    await self._listening_task
                
                elif self.state == ConversationState.PROCESSING:
                    # å¤„ç†è¯­éŸ³è¾“å…¥
                    if self._current_turn:
                        self._processing_task = asyncio.create_task(
                            self._process_conversation_turn(self._current_turn)
                        )
                        await self._processing_task
                
                elif self.state == ConversationState.SPEAKING:
                    # ç­‰å¾…è¯­éŸ³æ’­æ”¾å®Œæˆ
                    await asyncio.sleep(0.1)
                
                else:
                    await asyncio.sleep(0.1)
                    
            except asyncio.CancelledError:
                self.logger.info("å¯¹è¯å¾ªç¯è¢«å–æ¶ˆ")
                break
            except Exception as e:
                self.logger.error(f"å¯¹è¯å¾ªç¯å‡ºé”™: {e}")
                self._set_state(ConversationState.ERROR)
                await asyncio.sleep(1.0)  # é”™è¯¯æ¢å¤ç­‰å¾…
                self._set_state(ConversationState.IDLE)
    
    async def _listen_for_speech(self):
        """ç›‘å¬è¯­éŸ³è¾“å…¥"""
        self.logger.debug("å¼€å§‹ç›‘å¬è¯­éŸ³...")
        
        # è®¾ç½®ç©ºé—²åŠ¨ç”»
        if self.live2d_manager:
            await self.live2d_manager.set_idle_animation()
        
        speech_detected = False
        speech_buffer = []
        silence_start = None
        speech_start = None
        
        try:
            while self.state == ConversationState.LISTENING:
                # è¿™é‡Œåº”è¯¥ä»éŸ³é¢‘ç®¡ç†å™¨è·å–å®æ—¶éŸ³é¢‘å—
                # æš‚æ—¶ä½¿ç”¨æ¨¡æ‹Ÿçš„ç­‰å¾…
                await asyncio.sleep(0.1)
                
                # TODO: å®é™…çš„éŸ³é¢‘å¤„ç†é€»è¾‘
                # audio_chunk = await self.audio_manager.get_audio_chunk()
                # has_speech = await self.asr.detect_voice_activity(audio_chunk)
                
                # æ¨¡æ‹Ÿè¯­éŸ³æ´»åŠ¨æ£€æµ‹
                has_speech = False  # è¿™é‡Œéœ€è¦å®é™…çš„ VAD æ£€æµ‹
                
                if has_speech:
                    if not speech_detected:
                        # å¼€å§‹è¯´è¯
                        speech_detected = True
                        speech_start = time.time()
                        self.logger.debug("æ£€æµ‹åˆ°è¯­éŸ³æ´»åŠ¨")
                    
                    silence_start = None
                    # speech_buffer.append(audio_chunk)
                    
                    # æ£€æŸ¥æœ€å¤§è¯­éŸ³æ—¶é•¿
                    if speech_start and (time.time() - speech_start) > self.max_speech_duration:
                        self.logger.warning("è¯­éŸ³è¾“å…¥è¶…æ—¶ï¼Œå¼ºåˆ¶ç»“æŸ")
                        break
                
                elif speech_detected:
                    # è¯´è¯ä¸­çš„é™éŸ³
                    if silence_start is None:
                        silence_start = time.time()
                    elif (time.time() - silence_start) > self.silence_timeout:
                        # é™éŸ³è¶…æ—¶ï¼Œç»“æŸè¯­éŸ³è¾“å…¥
                        self.logger.debug("æ£€æµ‹åˆ°è¯­éŸ³ç»“æŸ")
                        break
            
            # å¤„ç†æ”¶é›†åˆ°çš„è¯­éŸ³æ•°æ®
            if speech_detected and speech_buffer:
                await self._handle_speech_input(speech_buffer)
            
        except asyncio.CancelledError:
            self.logger.debug("è¯­éŸ³ç›‘å¬è¢«å–æ¶ˆ")
        except Exception as e:
            self.logger.error(f"è¯­éŸ³ç›‘å¬å‡ºé”™: {e}")
            self._set_state(ConversationState.ERROR)
    
    async def _handle_speech_input(self, speech_buffer):
        """å¤„ç†è¯­éŸ³è¾“å…¥"""
        try:
            # åˆå¹¶éŸ³é¢‘å—
            combined_audio = b''.join(speech_buffer)
            
            # å¯åŠ¨æ–°çš„å¯¹è¯è½®æ¬¡
            turn_start = time.time()
            
            # è¯­éŸ³è¯†åˆ«
            asr_start = time.time()
            recognized_text = await self.asr.transcribe(combined_audio)
            asr_time = time.time() - asr_start
            
            if not recognized_text or recognized_text.strip() == "":
                self.logger.debug("æœªè¯†åˆ«åˆ°æœ‰æ•ˆæ–‡æœ¬")
                self._set_state(ConversationState.IDLE)
                return
            
            self.logger.info(f"è¯†åˆ«åˆ°è¯­éŸ³: {recognized_text}")
            
            # åˆ›å»ºå¯¹è¯è½®æ¬¡
            self._current_turn = ConversationTurn(
                user_input=recognized_text,
                assistant_response="",
                timestamp=turn_start,
                asr_latency=asr_time
            )
            
            # è§¦å‘å›è°ƒ
            if self.on_user_speech:
                self.on_user_speech(recognized_text)
            
            # åˆ‡æ¢åˆ°å¤„ç†çŠ¶æ€
            self._set_state(ConversationState.PROCESSING)
            
        except Exception as e:
            self.logger.error(f"å¤„ç†è¯­éŸ³è¾“å…¥å¤±è´¥: {e}")
            self._set_state(ConversationState.ERROR)
    
    async def _process_conversation_turn(self, turn: ConversationTurn):
        """å¤„ç†å®Œæ•´çš„å¯¹è¯è½®æ¬¡"""
        try:
            self.logger.info(f"å¤„ç†ç”¨æˆ·è¾“å…¥: {turn.user_input}")
            
            # LLM ç”Ÿæˆå›åº”
            llm_start = time.time()
            response_text = await self._generate_llm_response(turn.user_input)
            turn.llm_latency = time.time() - llm_start
            
            if not response_text:
                self.logger.warning("LLM æœªç”Ÿæˆæœ‰æ•ˆå›åº”")
                self._set_state(ConversationState.IDLE)
                return
            
            # è§£ææƒ…æ„Ÿæ ‡ç­¾
            emotion, clean_text = self._parse_emotion_tags(response_text)
            turn.assistant_response = clean_text
            turn.emotion_detected = emotion
            
            self.logger.info(f"ç”Ÿæˆå›åº”: {clean_text}")
            if emotion:
                self.logger.info(f"æ£€æµ‹åˆ°æƒ…æ„Ÿ: {emotion}")
            
            # è§¦å‘å›è°ƒ
            if self.on_assistant_response:
                self.on_assistant_response(clean_text)
            if emotion and self.on_emotion_detected:
                self.on_emotion_detected(emotion)
            
            # è®¾ç½®æƒ…æ„ŸåŠ¨ç”»
            if emotion and self.live2d_manager:
                await self._set_emotion_animation(emotion)
            
            # è¯­éŸ³åˆæˆ
            self._set_state(ConversationState.RESPONDING)
            tts_start = time.time()
            audio_data = await self.tts.synthesize(clean_text)
            turn.tts_latency = time.time() - tts_start
            
            if audio_data:
                # æ’­æ”¾è¯­éŸ³å¹¶åŒæ­¥åŠ¨ç”»
                self._set_state(ConversationState.SPEAKING)
                await self._play_speech_with_animation(audio_data, clean_text)
            
            # è®¡ç®—æ€»å»¶è¿Ÿ
            turn.total_latency = time.time() - turn.timestamp
            
            # æ·»åŠ åˆ°å†å²è®°å½•
            self.conversation_history.append(turn)
            if len(self.conversation_history) > self.max_history_turns:
                self.conversation_history = self.conversation_history[-self.max_history_turns:]
            
            # è§¦å‘å›è°ƒ
            if self.on_turn_completed:
                self.on_turn_completed(turn)
            
            self.logger.info(f"å¯¹è¯è½®æ¬¡å®Œæˆï¼Œæ€»å»¶è¿Ÿ: {turn.total_latency:.2f}s")
            
            # è¿”å›ç©ºé—²çŠ¶æ€
            self._current_turn = None
            self._set_state(ConversationState.IDLE)
            
        except Exception as e:
            self.logger.error(f"å¤„ç†å¯¹è¯è½®æ¬¡å¤±è´¥: {e}")
            self._set_state(ConversationState.ERROR)
    
    async def _generate_llm_response(self, user_input: str) -> str:
        """ç”Ÿæˆ LLM å›åº”"""
        try:
            # æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡
            messages = self._build_conversation_context(user_input)
            
            # è°ƒç”¨ LLM
            response = await self.llm.chat_completion(messages)
            return response
            
        except Exception as e:
            self.logger.error(f"LLM ç”Ÿæˆå›åº”å¤±è´¥: {e}")
            return ""
    
    def _build_conversation_context(self, user_input: str) -> List[Dict[str, str]]:
        """æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡"""
        messages = []
        
        # ç³»ç»Ÿæç¤º - Hiyori è§’è‰²è®¾å®š
        system_prompt = """ä½ æ˜¯ Hiyoriï¼Œä¸€ä¸ªæ´»æ³¼å¯çˆ±çš„è™šæ‹Ÿæ•°å­—äººã€‚ä½ å…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

1. æ€§æ ¼ï¼šå¼€æœ—ã€å‹å–„ã€å……æ»¡æ´»åŠ›ï¼Œå¶å°”ä¼šæœ‰ç‚¹è°ƒçš®
2. è¯´è¯é£æ ¼ï¼šäº²åˆ‡è‡ªç„¶ï¼Œå–œæ¬¢ä½¿ç”¨è¡¨æƒ…ç¬¦å·è¡¨è¾¾æƒ…æ„Ÿ
3. æƒ…æ„Ÿè¡¨è¾¾ï¼šé€šè¿‡ [EMOTION:emoji] æ ‡ç­¾æ¥è¡¨è¾¾æƒ…æ„Ÿï¼Œæ¯”å¦‚ï¼š
   - å¼€å¿ƒæ—¶ä½¿ç”¨ [EMOTION:ğŸ˜Š]
   - æ€è€ƒæ—¶ä½¿ç”¨ [EMOTION:ğŸ¤”]
   - å…´å¥‹æ—¶ä½¿ç”¨ [EMOTION:ğŸ¤©]
   - éš¾è¿‡æ—¶ä½¿ç”¨ [EMOTION:ğŸ˜¢]

è¯·æ ¹æ®å¯¹è¯å†…å®¹é€‚å½“ä½¿ç”¨æƒ…æ„Ÿæ ‡ç­¾ï¼Œè®©äº¤æµæ›´åŠ ç”ŸåŠ¨æœ‰è¶£ã€‚"""
        
        messages.append({"role": "system", "content": system_prompt})
        
        # æ·»åŠ å†å²å¯¹è¯
        for turn in self.conversation_history[-5:]:  # åªä¿ç•™æœ€è¿‘5è½®å¯¹è¯
            messages.append({"role": "user", "content": turn.user_input})
            messages.append({"role": "assistant", "content": turn.assistant_response})
        
        # æ·»åŠ å½“å‰ç”¨æˆ·è¾“å…¥
        messages.append({"role": "user", "content": user_input})
        
        return messages
    
    def _parse_emotion_tags(self, text: str) -> tuple[Optional[str], str]:
        """è§£ææƒ…æ„Ÿæ ‡ç­¾"""
        # æŸ¥æ‰¾æƒ…æ„Ÿæ ‡ç­¾
        emotion_pattern = r'\[EMOTION:([^\]]+)\]'
        matches = re.findall(emotion_pattern, text)
        
        # æ¸…ç†æ–‡æœ¬
        clean_text = re.sub(emotion_pattern, '', text).strip()
        
        # è¿”å›æœ€åä¸€ä¸ªæƒ…æ„Ÿæ ‡ç­¾ï¼ˆå¦‚æœæœ‰å¤šä¸ªï¼‰
        emotion = matches[-1] if matches else None
        
        return emotion, clean_text
    
    async def _set_emotion_animation(self, emotion: str):
        """è®¾ç½®æƒ…æ„ŸåŠ¨ç”»"""
        try:
            if emotion in self.emotion_mapping:
                mapping = self.emotion_mapping[emotion]
                
                # è®¾ç½®è¡¨æƒ…
                if mapping.get("expression"):
                    await self.live2d_manager.set_expression(mapping["expression"])
                
                # æ’­æ”¾åŠ¨ä½œ
                if mapping.get("motion_group"):
                    await self.live2d_manager.play_motion(mapping["motion_group"])
                    
                self.logger.debug(f"åº”ç”¨æƒ…æ„ŸåŠ¨ç”»: {emotion}")
            else:
                self.logger.warning(f"æœªçŸ¥çš„æƒ…æ„Ÿæ ‡ç­¾: {emotion}")
                
        except Exception as e:
            self.logger.error(f"è®¾ç½®æƒ…æ„ŸåŠ¨ç”»å¤±è´¥: {e}")
    
    async def _play_speech_with_animation(self, audio_data: bytes, text: str):
        """æ’­æ”¾è¯­éŸ³å¹¶åŒæ­¥åŠ¨ç”»"""
        try:
            # è·å–éŸ³ç´ æ—¶åºä¿¡æ¯ç”¨äºå”‡å½¢åŒæ­¥
            phoneme_timing = await self.tts.get_phoneme_timing(text)
            
            # å¯åŠ¨å”‡å½¢åŒæ­¥ä»»åŠ¡
            lipsync_task = None
            if self.live2d_manager and phoneme_timing:
                lipsync_task = asyncio.create_task(
                    self.live2d_manager.sync_lipsync(phoneme_timing)
                )
            
            # æ’­æ”¾éŸ³é¢‘
            await self.tts.play_audio(audio_data)
            
            # ç­‰å¾…å”‡å½¢åŒæ­¥å®Œæˆ
            if lipsync_task:
                await lipsync_task
                
        except Exception as e:
            self.logger.error(f"æ’­æ”¾è¯­éŸ³å’ŒåŠ¨ç”»å¤±è´¥: {e}")
    
    async def process_audio(self, audio_data: bytes):
        """å¤„ç†éŸ³é¢‘æ•°æ®ï¼ˆä»å¤–éƒ¨è°ƒç”¨ï¼‰"""
        if self.state == ConversationState.LISTENING:
            # è¿™é‡Œå¯ä»¥ç´¯ç§¯éŸ³é¢‘æ•°æ®è¿›è¡Œ VAD æ£€æµ‹
            # å¦‚æœæ£€æµ‹åˆ°è¯­éŸ³ï¼Œåˆ™åˆ‡æ¢çŠ¶æ€
            pass
    
    async def process_text_input(self, text: str) -> str:
        """å¤„ç†æ–‡æœ¬è¾“å…¥ï¼ˆç”¨äºè°ƒè¯•æˆ–æ–‡æœ¬èŠå¤©ï¼‰"""
        try:
            # åˆ›å»ºä¸´æ—¶å¯¹è¯è½®æ¬¡
            turn = ConversationTurn(
                user_input=text,
                assistant_response="",
                timestamp=time.time()
            )
            
            # ç”Ÿæˆå›åº”
            response = await self._generate_llm_response(text)
            emotion, clean_response = self._parse_emotion_tags(response)
            
            turn.assistant_response = clean_response
            turn.emotion_detected = emotion
            
            # è®¾ç½®æƒ…æ„ŸåŠ¨ç”»
            if emotion and self.live2d_manager:
                await self._set_emotion_animation(emotion)
            
            # æ·»åŠ åˆ°å†å²
            self.conversation_history.append(turn)
            if len(self.conversation_history) > self.max_history_turns:
                self.conversation_history = self.conversation_history[-self.max_history_turns:]
            
            return clean_response
            
        except Exception as e:
            self.logger.error(f"å¤„ç†æ–‡æœ¬è¾“å…¥å¤±è´¥: {e}")
            return "æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•å›åº”ã€‚"
    
    def _set_state(self, new_state: ConversationState):
        """è®¾ç½®å¯¹è¯çŠ¶æ€"""
        if self.state != new_state:
            old_state = self.state
            self.state = new_state
            self.logger.debug(f"å¯¹è¯çŠ¶æ€å˜æ›´: {old_state.value} -> {new_state.value}")
            
            if self.on_state_changed:
                self.on_state_changed(new_state)
    
    # å…¬å…±æ¥å£
    def get_conversation_history(self) -> List[ConversationTurn]:
        """è·å–å¯¹è¯å†å²"""
        return self.conversation_history.copy()
    
    def get_current_state(self) -> ConversationState:
        """è·å–å½“å‰çŠ¶æ€"""
        return self.state
    
    def clear_history(self):
        """æ¸…ç©ºå¯¹è¯å†å²"""
        self.conversation_history.clear()
        self.logger.info("å¯¹è¯å†å²å·²æ¸…ç©º")
    
    async def interrupt_current_turn(self):
        """ä¸­æ–­å½“å‰å¯¹è¯è½®æ¬¡"""
        try:
            # å–æ¶ˆå½“å‰ä»»åŠ¡
            if self._listening_task and not self._listening_task.done():
                self._listening_task.cancel()
            
            if self._processing_task and not self._processing_task.done():
                self._processing_task.cancel()
            
            if self._speaking_task and not self._speaking_task.done():
                self._speaking_task.cancel()
            
            # åœæ­¢éŸ³é¢‘æ’­æ”¾
            if self.tts:
                await self.tts.stop_playback()
            
            # é‡ç½®çŠ¶æ€
            self._current_turn = None
            self._set_state(ConversationState.IDLE)
            
            self.logger.info("å½“å‰å¯¹è¯è½®æ¬¡å·²ä¸­æ–­")
            
        except Exception as e:
            self.logger.error(f"ä¸­æ–­å¯¹è¯è½®æ¬¡å¤±è´¥: {e}") 